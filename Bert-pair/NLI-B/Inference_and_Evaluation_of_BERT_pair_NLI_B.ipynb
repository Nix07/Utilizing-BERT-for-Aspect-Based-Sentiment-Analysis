{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Inference_and_Evaluation_of_BERT_pair_NLI_B.ipynb","provenance":[],"collapsed_sections":["LACVADTH_U8N","qG4vUkDMGtZ6","Bapw7CmTT38f","2m6lnQaklouY"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"e2f747484db74fb9af684406c611ed0a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_1b1504d195104db6911be37e1ec9b772","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a24c17762f1045129d9fee42ae0a4aac","IPY_MODEL_dce1516900644d3fa1a7fd3c5ab81733"]}},"1b1504d195104db6911be37e1ec9b772":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a24c17762f1045129d9fee42ae0a4aac":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b640d8e67ec245668ae221cb0c9954a1","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_db2902b0212c444487cfd5c4330946ac"}},"dce1516900644d3fa1a7fd3c5ab81733":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b0d9966856ee47ab9c56c38ac942d95d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:00&lt;00:00, 2.13MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c6342d9f855549f8953911f7b302979f"}},"b640d8e67ec245668ae221cb0c9954a1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"db2902b0212c444487cfd5c4330946ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b0d9966856ee47ab9c56c38ac942d95d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c6342d9f855549f8953911f7b302979f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"hIANK2Rn_AS2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":649},"executionInfo":{"status":"ok","timestamp":1596498982417,"user_tz":-330,"elapsed":34050,"user":{"displayName":"NIKHIL PRAKASH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE64-zyQNQSjSixENWJyhGqao6rzutp-TYuXNn2Q=s64","userId":"07464489639094286985"}},"outputId":"685cd710-3e0a-43dd-9ace-68e42371389c"},"source":["# Install dependencies\n","!pip uninstall -y tensorflow\n","!pip install transformers"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Uninstalling tensorflow-2.3.0:\n","  Successfully uninstalled tensorflow-2.3.0\n","Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n","\u001b[K     |████████████████████████████████| 778kB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Collecting tokenizers==0.8.1.rc1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 10.1MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Collecting sentencepiece!=0.1.92\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 30.4MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 34.5MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=aa5a5bc45a01d6a36ded9f351e8d1e90df8c3d8d5d37f747a231924783c14572\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0uIEvh0A_FUk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1596499079565,"user_tz":-330,"elapsed":95751,"user":{"displayName":"NIKHIL PRAKASH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE64-zyQNQSjSixENWJyhGqao6rzutp-TYuXNn2Q=s64","userId":"07464489639094286985"}},"outputId":"32f597da-472a-4108-da9c-5724df57f9a2"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GK_-dM26_TGi","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596499084010,"user_tz":-330,"elapsed":5189,"user":{"displayName":"NIKHIL PRAKASH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE64-zyQNQSjSixENWJyhGqao6rzutp-TYuXNn2Q=s64","userId":"07464489639094286985"}}},"source":["import json\n","import torch\n","import torch.nn as nn\n","import pandas as pd\n","import numpy as np\n","import transformers\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","from sklearn.metrics import f1_score, accuracy_score\n","from transformers import BertModel, BertTokenizer\n","\n","import logging\n","logging.basicConfig(level=logging.ERROR)\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"VGTic4EP_XSG","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596499084013,"user_tz":-330,"elapsed":3628,"user":{"displayName":"NIKHIL PRAKASH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE64-zyQNQSjSixENWJyhGqao6rzutp-TYuXNn2Q=s64","userId":"07464489639094286985"}}},"source":["class SentimentClassifier(nn.Module):\n","  \"\"\"\n","  This class defines the model architecture which is simply a fully-connected\n","  layer on top of a pre-trained BERT model. \n","  \"\"\"\n","\n","  def __init__(self, BERT_MODEL):\n","    super(SentimentClassifier, self).__init__()\n","    self.bert = BertModel.from_pretrained(BERT_MODEL)\n","    self.drop = nn.Dropout(p=0.3)\n","    self.out = nn.Linear(self.bert.config.hidden_size, 2) # Binary Classifier\n","\n","  def forward(self, ids, mask, token_type_ids):\n","    last_hidden_state, pooled_output = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n","    output = self.drop(pooled_output)\n","    return self.out(output)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LACVADTH_U8N","colab_type":"text"},"source":["# Inference on BERT-pair NLI-B"]},{"cell_type":"code","metadata":{"id":"hDO650jp_Z7X","colab_type":"code","colab":{}},"source":["class SentiHood:\n","  \"\"\"\n","  This class tokenizes the input text using the pre-trained BERT tokenizer \n","  (wordpiece) and returns the corresponding tensors.\n","  \"\"\"\n","  \n","  def __init__(self, opinions_id, text, auxiliary_sentence, targets, tokenizer, max_len):\n","    self.opinions_id = opinions_id\n","    self.text = text\n","    self.auxiliary_sentence = auxiliary_sentence\n","    self.tokenizer = tokenizer\n","    self.max_len = max_len\n","    self.targets = targets\n","\n","  def __len__(self):\n","    return len(self.targets)\n","\n","  def __getitem__(self, item):\n","    opinions_id = self.opinions_id[item]\n","    text = str(self.text[item])\n","    auxiliary_sentence = str(self.auxiliary_sentence[item])\n","    targets = self.targets[item]\n","\n","    text = text + ' ' + auxiliary_sentence\n","\n","    inputs = self.tokenizer.encode_plus(\n","        text,\n","        add_special_tokens = True,\n","        max_length = self.max_len,\n","        pad_to_max_length = True\n","    )\n","\n","    ids = inputs[\"input_ids\"]\n","    mask = inputs[\"attention_mask\"]\n","    token_type_ids = inputs[\"token_type_ids\"]\n","\n","    return {\n","        \"ids\": torch.tensor(ids, dtype=torch.long),\n","        \"mask\": torch.tensor(mask, dtype=torch.long),\n","        \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n","        \"targets\": torch.tensor(targets, dtype=torch.long),\n","        \"opinions_id\": torch.tensor(opinions_id, dtype=torch.long)\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tc_tUUJABEVB","colab_type":"code","colab":{}},"source":["def infer_loop_function(data_loader, model, device):\n","  \"\"\"\n","  This function performs the inference on testing sets and stores the predicted\n","  values.\n","  \"\"\"\n","\n","  model.eval()\n","\n","  df_pred = pd.DataFrame({\"id\": [], \"probability\": [], \"actual\": []})\n","\n","  ii = 0\n","  for bi, d in tqdm(enumerate(data_loader), total=len(data_loader), ncols=80):\n","    opinions_id = d[\"opinions_id\"]\n","    ids = d[\"ids\"]\n","    mask = d[\"mask\"]\n","    token_type_ids = d[\"token_type_ids\"]\n","    targets = d[\"targets\"]\n","\n","    opinions_id = opinions_id.to(device, dtype=torch.long)\n","    ids = ids.to(device, dtype=torch.long)\n","    mask = mask.to(device, dtype=torch.long)\n","    token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","    targets = targets.to(device, dtype=torch.long)\n","\n","    outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n","    outputs = F.softmax(outputs, dim=-1)\n","\n","    outputs = outputs.detach().cpu().numpy()\n","    targets = targets.detach().cpu().numpy()\n","    opinions_id = opinions_id.detach().cpu().numpy()\n","\n","    # Storing the probability of \"yes\" as the matching score.\n","    for k in range(len(outputs)):\n","      df_pred.loc[ii] = [str(opinions_id[k]), outputs[k][1], targets[k]]\n","      ii += 1\n","\n","  df_pred.to_csv('/content/drive/My Drive/SentiHood/Bert-pair/NLI-B/PredictedValues.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xalt3-vB_doq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"a2bc8aa0-d11a-45d3-a408-60c1f8d86406"},"source":["def run():\n","  \"\"\"\n","  This function defines the necessary hyperparameters and models. It also \n","  loads and tokenizes the testing dataset and execute the inference procedure.\n","  \"\"\"\n","\n","  TRAIN_MAX_LEN = 160\n","  TRAIN_BATCH_SIZE = 16\n","  BERT_MODEL = 'bert-base-uncased'\n","\n","  testing_set_path = '/content/drive/My Drive/SentiHood/Bert-pair/NLI-B/Datasets/testing_set.csv'\n","\n","  df_test = pd.read_csv(testing_set_path)\n","  df_test = df_test.reset_index(drop=True)\n","\n","  tokenizer = transformers.BertTokenizer.from_pretrained(BERT_MODEL)\n","\n","  test_dataset = SentiHood(\n","      opinions_id = df_test['id'].values,\n","      text = df_test['text'].values,\n","      auxiliary_sentence = df_test['auxiliary_sentence'],\n","      targets = df_test['sentiment'].values,\n","      tokenizer = tokenizer,\n","      max_len = TRAIN_MAX_LEN\n","  )\n","  print(f\"Testing Set: {len(test_dataset)}\")\n","\n","  test_data_loader = torch.utils.data.DataLoader(\n","      test_dataset,\n","      batch_size = TRAIN_BATCH_SIZE,\n","      shuffle=False\n","  )\n","\n","  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","  print(f\"Device: {device}\")\n","\n","  model = torch.load('/content/drive/My Drive/SentiHood/Bert-pair/NLI-B/Models/best.bin')\n","  infer_loop_function(data_loader=test_data_loader, model=model, device=device)\n","      \n","if __name__ == \"__main__\":\n","  run()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Testing Set: 67644\n","Device: cuda:0\n"],"name":"stdout"},{"output_type":"stream","text":["100%|███████████████████████████████████████| 4228/4228 [15:48<00:00,  4.46it/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"qG4vUkDMGtZ6","colab_type":"text"},"source":["# Evaluation of BERT-pair NLI-B"]},{"cell_type":"code","metadata":{"id":"5XAgZ22Jav1E","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1596499199986,"user_tz":-330,"elapsed":91174,"user":{"displayName":"NIKHIL PRAKASH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE64-zyQNQSjSixENWJyhGqao6rzutp-TYuXNn2Q=s64","userId":"07464489639094286985"}},"outputId":"d602c750-fc61-4f93-80c4-74ce048286df"},"source":["\"\"\"\n","Select the sequence with highest matching score, to compute the sentiment.\n","\"\"\"\n","\n","df_NLIB = pd.read_csv('/content/drive/My Drive/SentiHood/Bert-pair/NLI-B/PredictedValues.csv')\n","df_NLIM = pd.read_csv('/content/drive/My Drive/SentiHood/Bert-pair/NLI-M/PredictedValues.csv')\n","\n","df = pd.DataFrame({\"id\": [], \"predicted\": []})\n","\n","ii = 0\n","for k in tqdm(range(0, df_NLIB.shape[0]-3, 3), ncols=80):\n","  positive_probability = df_NLIB.iloc[k]['probability']\n","  negative_probability = df_NLIB.iloc[k+1]['probability']\n","  none_probability = df_NLIB.iloc[k+2]['probability']\n","\n","  if positive_probability > negative_probability and positive_probability > none_probability:\n","    df.loc[ii] = [str(df_NLIB.iloc[k]['id']), 0]\n","\n","  elif negative_probability > positive_probability and negative_probability > none_probability:\n","    df.loc[ii] = [str(df_NLIB.iloc[k]['id']), 1]\n","\n","  else:\n","    df.loc[ii] = [str(df_NLIB.iloc[k]['id']), 2]\n","  \n","  ii += 1\n","\n","df['predicted'] = df['predicted'].astype(int)\n","df['actual'] = df_NLIM['actual']"],"execution_count":5,"outputs":[{"output_type":"stream","text":["100%|████████████████████████████████████| 22547/22547 [01:28<00:00, 255.86it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"HtYO_f4pGyJY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"0fe1eda2-c18b-422d-a1e5-0f47a832c713"},"source":["def compute_sentiment_accuracy(df):\n","  \"\"\"This function computes the sentiment classfication accuracy\"\"\"\n","  \n","  accuracy = df[df['predicted'] == df['actual']].shape[0]/df.shape[0] * 100\n","  return round(accuracy, 2)\n","\n","print(f'Sentiment Accuracy of BERT-pair NLI-B = {compute_sentiment_accuracy(df)}%')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Sentiment Accuracy of BERT-pair NLI-B = 96.66%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TzIfR5nxKbv9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"dd026da3-425e-4a59-9a4d-5c39b6f481a4"},"source":["def compute_aspect_f1_score(df):\n","  \"\"\"\n","  This function computest the macro F1 score of predicted aspects.\n","  0 => Represents that the aspect has not been detected.\n","  1 => Represents that the aspect has been detected.\n","  \"\"\"\n","  \n","  df = df.replace([0, 1], 1).replace(2, 0)\n","\n","  total_f1_score = 0\n","  total = 0\n","  \n","  for i in range(0, df.shape[0], 12):\n","    true_values = df.iloc[i:i+12]['predicted']\n","    predicted_values = df.iloc[i:i+12]['actual']\n","\n","    total_f1_score += f1_score(true_values, predicted_values, average=\"macro\")\n","    total += 1\n","\n","  score = float(total_f1_score)/float(total)*100\n","  return round(score, 2)\n","\n","print(f\"Aspect F1 score: {compute_aspect_f1_score(df)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Aspect F1 score: 86.51\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"icSi6UoPHLAG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"d4241197-b687-431b-a876-c6b1381e6d1b"},"source":["def compute_aspect_accuracy(df):\n","  \"\"\"\n","  This function computes the strict aspect accuracy.\n","  0 => Represents that the aspect has not been detected.\n","  1 => Represents that the aspect has been detected.\n","  \"\"\"\n","  \n","  df = df.replace([0, 1], 1).replace(2, 0)\n","\n","  count = 0\n","  total = 0\n","\n","  for i in range(0, df.shape[0], 12):\n","    true_values = df.iloc[i:i+12]['predicted']\n","    predicted_values = df.iloc[i:i+12]['actual']\n","\n","    if (true_values == predicted_values).all():\n","      count += 1\n","    total += 1\n","\n","  accuracy = float(count)/float(total)*100\n","  return round(accuracy, 2)\n","\n","print(f'Aspect Accuracy (strict) of BERT-pair NLI-B = {compute_aspect_accuracy(df)}%')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Aspect Accuracy (strict) of BERT-pair NLI-B = 68.28%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Bapw7CmTT38f","colab_type":"text"},"source":["# Prediction Result Analysis\n","\n","This section analyses the predicted results to find the aspects and sentiments that are most and least accurate.\n","\n","*Note*: Utilizing the fact that first 1491x12 entries in the **above** constructed `df` are related to `location-1` and rest are related to `location-2`. "]},{"cell_type":"code","metadata":{"id":"R090gFrsq3lO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596499216203,"user_tz":-330,"elapsed":13775,"user":{"displayName":"NIKHIL PRAKASH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE64-zyQNQSjSixENWJyhGqao6rzutp-TYuXNn2Q=s64","userId":"07464489639094286985"}},"outputId":"a6efe67a-552c-43a0-bd7c-83a7a23bb6fe"},"source":["\"\"\"\n","Computes the positive correct, positive total, negative correct, negative total, \n","none correct, none total corresponding to all the aspects of LOCATION1.\n","\"\"\"\n","\n","aspects = ['dining', 'general', 'green-nature', 'live', 'multicultural', 'nightlife', 'price', 'quiet', 'safety', 'shopping', 'touristy', 'transit-location']\n","location1_aspects_result_analysis = {}\n","\n","for i in range(12):\n","  location1_aspects_result_analysis[aspects[i]] = [[0 ,0], [0 ,0], [0 ,0]]\n","\n","for i in tqdm(range(0, df['id'].unique().shape[0]*12-12, 12), ncols=80):\n","  for j in range(12):\n","    if df.loc[i+j]['actual'] == df.loc[i+j]['predicted']:\n","      location1_aspects_result_analysis[aspects[j]][int(df.loc[i+j]['actual'])][0] += 1\n","    \n","    location1_aspects_result_analysis[aspects[j]][int(df.loc[i+j]['actual'])][1] += 1"],"execution_count":6,"outputs":[{"output_type":"stream","text":["100%|██████████████████████████████████████| 1490/1490 [00:12<00:00, 115.93it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"v10o29bn6ghR","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596499216204,"user_tz":-330,"elapsed":13763,"user":{"displayName":"NIKHIL PRAKASH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE64-zyQNQSjSixENWJyhGqao6rzutp-TYuXNn2Q=s64","userId":"07464489639094286985"}}},"source":["df_location_aspect = pd.DataFrame({\"location\": [], \"aspect\": [], \"positive correct\": [], \"positive total\": [], \"negative correct\": [], \"negative total\": [], \"none correct\": [], \"none total\": [],})\n","\n","ii = 0\n","for key in location1_aspects_result_analysis.keys():\n","  df_location_aspect.loc[ii] = ['LOCATION1', f\"{key}\", \n","                                location1_aspects_result_analysis[key][0][0], \n","                                location1_aspects_result_analysis[key][0][1], \n","                                location1_aspects_result_analysis[key][1][0], \n","                                location1_aspects_result_analysis[key][1][1], \n","                                location1_aspects_result_analysis[key][2][0], \n","                                location1_aspects_result_analysis[key][2][1]]\n","  ii += 1"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"fEt5ghsD46Sp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596499219790,"user_tz":-330,"elapsed":17321,"user":{"displayName":"NIKHIL PRAKASH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE64-zyQNQSjSixENWJyhGqao6rzutp-TYuXNn2Q=s64","userId":"07464489639094286985"}},"outputId":"00730bfc-6c79-476a-891b-3009704d37e2"},"source":["\"\"\"\n","Computes the positive correct, positive total, negative correct, negative total, \n","none correct, none total corresponding to all the aspects of LOCATION2.\n","\"\"\"\n","\n","aspects = ['dining', 'general', 'green-nature', 'live', 'multicultural', 'nightlife', 'price', 'quiet', 'safety', 'shopping', 'touristy', 'transit-location']\n","location2_aspects_result_analysis = {}\n","\n","for i in range(12):\n","  location2_aspects_result_analysis[aspects[i]] = [[0 ,0], [0 ,0], [0 ,0]]\n","\n","for i in tqdm(range(df['id'].unique().shape[0]*12, df.shape[0]-12, 12), ncols=80):\n","  for j in range(12):\n","    if df.loc[i+j]['actual'] == df.loc[i+j]['predicted']:\n","      location2_aspects_result_analysis[aspects[j]][int(df.loc[i+j]['actual'])][0] += 1\n","    \n","    location2_aspects_result_analysis[aspects[j]][int(df.loc[i+j]['actual'])][1] += 1"],"execution_count":8,"outputs":[{"output_type":"stream","text":["100%|████████████████████████████████████████| 387/387 [00:03<00:00, 117.93it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"rEc6Qr612lvO","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596499219793,"user_tz":-330,"elapsed":17317,"user":{"displayName":"NIKHIL PRAKASH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE64-zyQNQSjSixENWJyhGqao6rzutp-TYuXNn2Q=s64","userId":"07464489639094286985"}}},"source":["for key in location2_aspects_result_analysis.keys():\n","  df_location_aspect.loc[ii] = ['LOCATION2', f\"{key}\", \n","                                location2_aspects_result_analysis[key][0][0], \n","                                location2_aspects_result_analysis[key][0][1], \n","                                location2_aspects_result_analysis[key][1][0], \n","                                location2_aspects_result_analysis[key][1][1], \n","                                location2_aspects_result_analysis[key][2][0], \n","                                location2_aspects_result_analysis[key][2][1]]\n","  ii += 1"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"qNwb2IaEXQcG","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596499219796,"user_tz":-330,"elapsed":17315,"user":{"displayName":"NIKHIL PRAKASH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE64-zyQNQSjSixENWJyhGqao6rzutp-TYuXNn2Q=s64","userId":"07464489639094286985"}}},"source":["df_location_aspect['positive percentage'] = round(df_location_aspect['positive correct']/df_location_aspect['positive total']*100, 2)\n","df_location_aspect['negative percentage'] = round(df_location_aspect['negative correct']/df_location_aspect['negative total']*100, 2)\n","df_location_aspect['none percentage'] = round(df_location_aspect['none correct']/df_location_aspect['none total']*100, 2)\n","\n","df_location_aspect['total percentage'] = round((df_location_aspect['positive correct'] + df_location_aspect['negative correct'] + df_location_aspect['none correct'])/(df_location_aspect['positive total'] + df_location_aspect['negative total'] + df_location_aspect['none total'])*100, 2)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"kA3cm8qMVxHo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":793},"executionInfo":{"status":"ok","timestamp":1596499219799,"user_tz":-330,"elapsed":17288,"user":{"displayName":"NIKHIL PRAKASH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE64-zyQNQSjSixENWJyhGqao6rzutp-TYuXNn2Q=s64","userId":"07464489639094286985"}},"outputId":"120f3f88-9372-4bce-f26c-3d2f4dd73719"},"source":["df_location_aspect"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>location</th>\n","      <th>aspect</th>\n","      <th>positive correct</th>\n","      <th>positive total</th>\n","      <th>negative correct</th>\n","      <th>negative total</th>\n","      <th>none correct</th>\n","      <th>none total</th>\n","      <th>positive percentage</th>\n","      <th>negative percentage</th>\n","      <th>none percentage</th>\n","      <th>total percentage</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>LOCATION1</td>\n","      <td>dining</td>\n","      <td>29.0</td>\n","      <td>30.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1453.0</td>\n","      <td>1458.0</td>\n","      <td>96.67</td>\n","      <td>50.00</td>\n","      <td>99.66</td>\n","      <td>99.53</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>LOCATION1</td>\n","      <td>general</td>\n","      <td>272.0</td>\n","      <td>359.0</td>\n","      <td>81.0</td>\n","      <td>113.0</td>\n","      <td>928.0</td>\n","      <td>1018.0</td>\n","      <td>75.77</td>\n","      <td>71.68</td>\n","      <td>91.16</td>\n","      <td>85.97</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>LOCATION1</td>\n","      <td>green-nature</td>\n","      <td>37.0</td>\n","      <td>40.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1433.0</td>\n","      <td>1450.0</td>\n","      <td>92.50</td>\n","      <td>NaN</td>\n","      <td>98.83</td>\n","      <td>98.66</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>LOCATION1</td>\n","      <td>live</td>\n","      <td>48.0</td>\n","      <td>63.0</td>\n","      <td>15.0</td>\n","      <td>23.0</td>\n","      <td>1361.0</td>\n","      <td>1404.0</td>\n","      <td>76.19</td>\n","      <td>65.22</td>\n","      <td>96.94</td>\n","      <td>95.57</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>LOCATION1</td>\n","      <td>multicultural</td>\n","      <td>31.0</td>\n","      <td>39.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>1441.0</td>\n","      <td>1448.0</td>\n","      <td>79.49</td>\n","      <td>33.33</td>\n","      <td>99.52</td>\n","      <td>98.86</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>LOCATION1</td>\n","      <td>nightlife</td>\n","      <td>57.0</td>\n","      <td>62.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1413.0</td>\n","      <td>1426.0</td>\n","      <td>91.94</td>\n","      <td>50.00</td>\n","      <td>99.09</td>\n","      <td>98.72</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>LOCATION1</td>\n","      <td>price</td>\n","      <td>66.0</td>\n","      <td>81.0</td>\n","      <td>103.0</td>\n","      <td>116.0</td>\n","      <td>1268.0</td>\n","      <td>1293.0</td>\n","      <td>81.48</td>\n","      <td>88.79</td>\n","      <td>98.07</td>\n","      <td>96.44</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>LOCATION1</td>\n","      <td>quiet</td>\n","      <td>7.0</td>\n","      <td>14.0</td>\n","      <td>12.0</td>\n","      <td>15.0</td>\n","      <td>1458.0</td>\n","      <td>1461.0</td>\n","      <td>50.00</td>\n","      <td>80.00</td>\n","      <td>99.79</td>\n","      <td>99.13</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>LOCATION1</td>\n","      <td>safety</td>\n","      <td>54.0</td>\n","      <td>61.0</td>\n","      <td>52.0</td>\n","      <td>66.0</td>\n","      <td>1336.0</td>\n","      <td>1363.0</td>\n","      <td>88.52</td>\n","      <td>78.79</td>\n","      <td>98.02</td>\n","      <td>96.78</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>LOCATION1</td>\n","      <td>shopping</td>\n","      <td>53.0</td>\n","      <td>62.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1421.0</td>\n","      <td>1427.0</td>\n","      <td>85.48</td>\n","      <td>100.00</td>\n","      <td>99.58</td>\n","      <td>98.99</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>LOCATION1</td>\n","      <td>touristy</td>\n","      <td>19.0</td>\n","      <td>25.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1455.0</td>\n","      <td>1465.0</td>\n","      <td>76.00</td>\n","      <td>NaN</td>\n","      <td>99.32</td>\n","      <td>98.93</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>LOCATION1</td>\n","      <td>transit-location</td>\n","      <td>118.0</td>\n","      <td>151.0</td>\n","      <td>11.0</td>\n","      <td>33.0</td>\n","      <td>1266.0</td>\n","      <td>1306.0</td>\n","      <td>78.15</td>\n","      <td>33.33</td>\n","      <td>96.94</td>\n","      <td>93.62</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>LOCATION2</td>\n","      <td>dining</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>378.0</td>\n","      <td>383.0</td>\n","      <td>75.00</td>\n","      <td>NaN</td>\n","      <td>98.69</td>\n","      <td>98.45</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>LOCATION2</td>\n","      <td>general</td>\n","      <td>67.0</td>\n","      <td>87.0</td>\n","      <td>20.0</td>\n","      <td>26.0</td>\n","      <td>247.0</td>\n","      <td>274.0</td>\n","      <td>77.01</td>\n","      <td>76.92</td>\n","      <td>90.15</td>\n","      <td>86.30</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>LOCATION2</td>\n","      <td>green-nature</td>\n","      <td>3.0</td>\n","      <td>7.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>374.0</td>\n","      <td>380.0</td>\n","      <td>42.86</td>\n","      <td>NaN</td>\n","      <td>98.42</td>\n","      <td>97.42</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>LOCATION2</td>\n","      <td>live</td>\n","      <td>11.0</td>\n","      <td>14.0</td>\n","      <td>3.0</td>\n","      <td>4.0</td>\n","      <td>357.0</td>\n","      <td>369.0</td>\n","      <td>78.57</td>\n","      <td>75.00</td>\n","      <td>96.75</td>\n","      <td>95.87</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>LOCATION2</td>\n","      <td>multicultural</td>\n","      <td>6.0</td>\n","      <td>8.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>376.0</td>\n","      <td>378.0</td>\n","      <td>75.00</td>\n","      <td>0.00</td>\n","      <td>99.47</td>\n","      <td>98.71</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>LOCATION2</td>\n","      <td>nightlife</td>\n","      <td>11.0</td>\n","      <td>12.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>371.0</td>\n","      <td>375.0</td>\n","      <td>91.67</td>\n","      <td>NaN</td>\n","      <td>98.93</td>\n","      <td>98.71</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>LOCATION2</td>\n","      <td>price</td>\n","      <td>21.0</td>\n","      <td>27.0</td>\n","      <td>23.0</td>\n","      <td>27.0</td>\n","      <td>321.0</td>\n","      <td>333.0</td>\n","      <td>77.78</td>\n","      <td>85.19</td>\n","      <td>96.40</td>\n","      <td>94.32</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>LOCATION2</td>\n","      <td>quiet</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>380.0</td>\n","      <td>380.0</td>\n","      <td>0.00</td>\n","      <td>60.00</td>\n","      <td>100.00</td>\n","      <td>98.97</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>LOCATION2</td>\n","      <td>safety</td>\n","      <td>9.0</td>\n","      <td>14.0</td>\n","      <td>13.0</td>\n","      <td>17.0</td>\n","      <td>350.0</td>\n","      <td>356.0</td>\n","      <td>64.29</td>\n","      <td>76.47</td>\n","      <td>98.31</td>\n","      <td>96.12</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>LOCATION2</td>\n","      <td>shopping</td>\n","      <td>11.0</td>\n","      <td>15.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>371.0</td>\n","      <td>372.0</td>\n","      <td>73.33</td>\n","      <td>NaN</td>\n","      <td>99.73</td>\n","      <td>98.71</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>LOCATION2</td>\n","      <td>touristy</td>\n","      <td>4.0</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>381.0</td>\n","      <td>382.0</td>\n","      <td>80.00</td>\n","      <td>NaN</td>\n","      <td>99.74</td>\n","      <td>99.48</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>LOCATION2</td>\n","      <td>transit-location</td>\n","      <td>17.0</td>\n","      <td>29.0</td>\n","      <td>1.0</td>\n","      <td>8.0</td>\n","      <td>337.0</td>\n","      <td>350.0</td>\n","      <td>58.62</td>\n","      <td>12.50</td>\n","      <td>96.29</td>\n","      <td>91.73</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     location            aspect  ...  none percentage  total percentage\n","0   LOCATION1            dining  ...            99.66             99.53\n","1   LOCATION1           general  ...            91.16             85.97\n","2   LOCATION1      green-nature  ...            98.83             98.66\n","3   LOCATION1              live  ...            96.94             95.57\n","4   LOCATION1     multicultural  ...            99.52             98.86\n","5   LOCATION1         nightlife  ...            99.09             98.72\n","6   LOCATION1             price  ...            98.07             96.44\n","7   LOCATION1             quiet  ...            99.79             99.13\n","8   LOCATION1            safety  ...            98.02             96.78\n","9   LOCATION1          shopping  ...            99.58             98.99\n","10  LOCATION1          touristy  ...            99.32             98.93\n","11  LOCATION1  transit-location  ...            96.94             93.62\n","12  LOCATION2            dining  ...            98.69             98.45\n","13  LOCATION2           general  ...            90.15             86.30\n","14  LOCATION2      green-nature  ...            98.42             97.42\n","15  LOCATION2              live  ...            96.75             95.87\n","16  LOCATION2     multicultural  ...            99.47             98.71\n","17  LOCATION2         nightlife  ...            98.93             98.71\n","18  LOCATION2             price  ...            96.40             94.32\n","19  LOCATION2             quiet  ...           100.00             98.97\n","20  LOCATION2            safety  ...            98.31             96.12\n","21  LOCATION2          shopping  ...            99.73             98.71\n","22  LOCATION2          touristy  ...            99.74             99.48\n","23  LOCATION2  transit-location  ...            96.29             91.73\n","\n","[24 rows x 12 columns]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"2m6lnQaklouY","colab_type":"text"},"source":["# Creating preds.jsonl\n","\n","This section constructs the `preds.jsonl` file which contains model predictions and original annotations in the following json format.\n","\n","\n","```\n","{\n","  \"opinions\": [\n","    {\n","      \"sentiment\": \"Positive\",\n","      \"aspect\": \"safety\",\n","      \"target_entity\": \"LOCATION1\"\n","    }\n","  ],\n","  \"id\": 153,\n","  \"text\": \" LOCATION1 is in Greater London and is a very safe place\",\n","  \"model_pred\": [\n","    {\n","      \"sentiment\": ...,\n","      \"aspect\": ...,\n","      \"target_entity\":...\n","    },...\n","  ]\n","}\n","```"]},{"cell_type":"code","metadata":{"id":"AVfjawsokj_t","colab_type":"code","colab":{}},"source":["with open('/content/drive/My Drive/SentiHood/SentiHood Dataset/sentihood-test.json', 'r') as fp:\n","  testing_set = json.load(fp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uQyXSIuUlyz_","colab_type":"code","colab":{}},"source":["labels_to_sentiment_dict = {\n","    0: 'Positive',\n","    1: 'Negative',\n","    2: 'None'\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2V00G2TJl4v2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":100,"referenced_widgets":["e2f747484db74fb9af684406c611ed0a","1b1504d195104db6911be37e1ec9b772","a24c17762f1045129d9fee42ae0a4aac","dce1516900644d3fa1a7fd3c5ab81733","b640d8e67ec245668ae221cb0c9954a1","db2902b0212c444487cfd5c4330946ac","b0d9966856ee47ab9c56c38ac942d95d","c6342d9f855549f8953911f7b302979f"]},"executionInfo":{"status":"ok","timestamp":1596445534786,"user_tz":-330,"elapsed":2492785,"user":{"displayName":"NIKHIL PRAKASH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE64-zyQNQSjSixENWJyhGqao6rzutp-TYuXNn2Q=s64","userId":"07464489639094286985"}},"outputId":"0940f70d-0c83-440b-fd19-89d9af08f92d"},"source":["BERT_MODEL = 'bert-base-uncased'\n","MAX_LEN = 160\n","locations = ['LOCATION1', 'LOCATION2']\n","aspects = ['dining', 'general', 'green-nature', 'live', 'multicultural', 'nightlife', 'price', 'quiet', 'safety', 'shopping', 'touristy', 'transit-location']\n","\n","tokenizer = transformers.BertTokenizer.from_pretrained(BERT_MODEL)\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Device: {device}\")\n","\n","model = torch.load('/content/drive/My Drive/SentiHood/Bert-pair/NLI-B/Models/best.bin')\n","\n","for each_example in tqdm(testing_set, ncols=80):\n","  id = each_example['id']\n","  text = each_example['text'].strip()\n","\n","  each_example['model_pred'] = []\n","\n","  count_location = 1\n","  for location in locations:\n","    if location in text:\n","       # If \"location\" is present in the text, then utilize the trained model\n","      # to predict the aspects and their corresponding sentiment of the text.\n","\n","      text = text.replace(location, 'location - ' + str(count_location))\n","      \n","      for aspect in aspects:\n","        predicted_probabilities = []\n","        aspect_sentiment = None\n","\n","        for polarity in ['Positive', 'Negative', 'None']:\n","          auxiliary_sentence = f\"the polarity of the aspect {aspect} of location - {str(count_location)} is {polarity}.\"\n","          \n","          combined_text = text + ' ' + auxiliary_sentence\n","          \n","          inputs = tokenizer.encode_plus(\n","              combined_text,\n","              add_special_tokens = True,\n","              max_length = MAX_LEN,\n","              pad_to_max_length = True\n","          )\n","          ids = torch.tensor(inputs[\"input_ids\"], dtype=torch.long).unsqueeze(0)\n","          mask = torch.tensor(inputs[\"attention_mask\"], dtype=torch.long).unsqueeze(0)\n","          token_type_ids = torch.tensor(inputs[\"token_type_ids\"], dtype=torch.long).unsqueeze(0)\n","\n","          ids = ids.to(device, dtype=torch.long)\n","          mask = mask.to(device, dtype=torch.long)\n","          token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","\n","          outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n","          outputs = F.softmax(outputs, dim=-1)\n","\n","          outputs = outputs.detach().cpu().numpy()\n","\n","          predicted_probabilities.append(outputs[0][1])\n","\n","        # Utilizing the matching score to compute the sentiment.\n","        if predicted_probabilities[0] > predicted_probabilities[1] and predicted_probabilities[0] > predicted_probabilities[2]:\n","          aspect_sentiment = 'Positive'\n","        elif predicted_probabilities[1] > predicted_probabilities[0] and predicted_probabilities[1] > predicted_probabilities[2]:\n","          aspect_sentiment = 'Negative'\n","        else:\n","          aspect_sentiment = None\n","\n","        # If predicted sentiment is not None, then add it to the preds.jsonl.\n","        \n","        if aspect_sentiment:\n","          result = {\n","              \"sentiment\": aspect_sentiment,\n","              \"aspect\": aspect,\n","              \"target_entity\": location\n","          }\n","          each_example['model_pred'].append(result)\n","      \n","    count_location += 1"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e2f747484db74fb9af684406c611ed0a","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Device: cuda:0\n"],"name":"stdout"},{"output_type":"stream","text":["100%|███████████████████████████████████████| 1491/1491 [41:16<00:00,  1.66s/it]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"LaWQo-rOa6tU","colab_type":"code","colab":{}},"source":["with open('/content/drive/My Drive/SentiHood/Bert-pair/NLI-B/preds.jsonl', mode='w', encoding='utf-8') as fp:\n","  for each in testing_set:\n","    json_record = json.dumps(each, ensure_ascii=False)\n","    fp.write(json_record + '\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JgbMhlGlTPxc","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}