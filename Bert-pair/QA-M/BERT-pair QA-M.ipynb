{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT-pair QA-M.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"0iwni_X20REP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":685},"executionInfo":{"status":"ok","timestamp":1596380888155,"user_tz":-330,"elapsed":47192,"user":{"displayName":"NIKHIL PRAKASH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE64-zyQNQSjSixENWJyhGqao6rzutp-TYuXNn2Q=s64","userId":"07464489639094286985"}},"outputId":"ab896794-4b01-40e3-ff36-c369cdc33aaa"},"source":["# Install dependencies\n","!pip uninstall -y tensorflow\n","!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Uninstalling tensorflow-2.2.0:\n","  Successfully uninstalled tensorflow-2.2.0\n","Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n","\u001b[K     |████████████████████████████████| 778kB 8.1MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Collecting tokenizers==0.8.1.rc1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 22.4MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Collecting sentencepiece!=0.1.92\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 53.7MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 60.9MB/s \n","\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=1406bd27d6c9d79c85860ca03064cd683fefbc321d5cbbec2d58570d0e52676c\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jg_IJ_vH99tF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":127},"executionInfo":{"status":"ok","timestamp":1596380926808,"user_tz":-330,"elapsed":24303,"user":{"displayName":"NIKHIL PRAKASH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE64-zyQNQSjSixENWJyhGqao6rzutp-TYuXNn2Q=s64","userId":"07464489639094286985"}},"outputId":"9f3e3ef5-bb61-46d8-b5d6-20529abc2c57"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WFxL89PU0bXV","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import transformers\n","from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n","from torch.optim import lr_scheduler\n","\n","import logging\n","logging.basicConfig(level=logging.ERROR)\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NoIu1VSSE_3U","colab_type":"code","colab":{}},"source":["class SentimentClassifier(nn.Module):\n","  \"\"\"\n","  This class defines the model architecture which is simply a fully-connected\n","  layer on top of a pre-trained BERT model. \n","  \"\"\"\n","\n","  def __init__(self, BERT_MODEL):\n","    super(SentimentClassifier, self).__init__()\n","    self.bert = BertModel.from_pretrained(BERT_MODEL)\n","    self.drop = nn.Dropout(p=0.3)\n","    self.out = nn.Linear(self.bert.config.hidden_size, 3) # Number of output classes = 3\n","\n","  def forward(self, ids, mask, token_type_ids):\n","    last_hidden_state, pooled_output = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n","    output = self.drop(pooled_output)\n","    return self.out(output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LySwsdB7-TxP","colab_type":"code","colab":{}},"source":["class SentiHood:\n","  \"\"\"\n","  This class tokenizes the input text using the pre-trained BERT tokenizer \n","  (wordpiece) and returns the corresponding tensors.\n","  \"\"\"\n","\n","  def __init__(self, text, auxiliary_sentence, targets, tokenizer, max_len):\n","    self.text = text\n","    self.auxiliary_sentence = auxiliary_sentence\n","    self.tokenizer = tokenizer\n","    self.max_len = max_len\n","    self.targets = targets\n","\n","  def __len__(self):\n","    return len(self.targets)\n","\n","  def __getitem__(self, item):\n","    text = str(self.text[item])\n","    auxiliary_sentence = str(self.auxiliary_sentence[item])\n","    targets = self.targets[item]\n","\n","    text = text + ' ' + auxiliary_sentence\n","\n","    inputs = self.tokenizer.encode_plus(\n","        text,\n","        add_special_tokens = True,\n","        max_length = self.max_len,\n","        pad_to_max_length = True\n","    )\n","\n","    ids = inputs[\"input_ids\"]\n","    mask = inputs[\"attention_mask\"]\n","    token_type_ids = inputs[\"token_type_ids\"]\n","\n","    return {\n","        \"ids\": torch.tensor(ids, dtype=torch.long),\n","        \"mask\": torch.tensor(mask, dtype=torch.long),\n","        \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n","        \"targets\": torch.tensor(targets, dtype=torch.long)\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NHt02ywTYH2A","colab_type":"code","colab":{}},"source":["def loss_function(outputs, targets):\n","\t\"\"\"\n","\tThis function defines the loss function which is used to train the model, i.e.\n","\tCrossEntropy.\n","\t\"\"\"\n","\n","\t# probability, predicted = torch.max(outputs, 1)\n","\t# print(f\"Predicted = {predicted.cpu().detach().numpy()}\\nTargets = {targets}\")\n","\n","\treturn nn.CrossEntropyLoss(reduction='mean')(outputs, targets)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QXP5g_SKWnW5","colab_type":"code","colab":{}},"source":["def train_loop_function(data_loader, model, optimizer, device):\n","  \"\"\"\n","  This function defines the training loop over the entire training set.\n","  \"\"\"\n","\n","  model.train()\n","\n","  running_loss = 0.0\n","  for bi, d in enumerate(data_loader):\n","    ids = d[\"ids\"]\n","    mask = d[\"mask\"]\n","    token_type_ids = d[\"token_type_ids\"]\n","    targets = d[\"targets\"]\n","\n","    ids = ids.to(device, dtype=torch.long)\n","    mask = mask.to(device, dtype=torch.long)\n","    token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","    targets = targets.to(device, dtype=torch.long)\n","\n","    optimizer.zero_grad()\n","\n","    outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n","    loss = loss_function(outputs, targets)\n","\n","    loss.backward()\n","    optimizer.step()\n","\n","    running_loss += loss.item()\n","    if bi % 10 == 0 and bi!=0:\n","      temp = f'Batch index = {bi}\\tLoss = {running_loss/10}'\n","      print(temp)\n","\n","      f1 = open('/content/drive/My Drive/SentiHood/Bert-pair/QA-M/Models/' + 'loss.txt', 'a+')\n","      temp = temp + '\\n'\n","      f1.write(temp)\n","      f1.close()\n","\n","      running_loss = 0.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rQ-TATCKXCyR","colab_type":"code","colab":{}},"source":["def eval_loop_function(data_loader, model, device):\n","  \"\"\"\n","  This function defines the evaluation loop over the entire validation set.\n","  It also computes accuracy of the trained model, which is used to select the \n","  best model.\n","  \"\"\"\n","\n","  model.eval()\n","\n","  corrects = 0\n","  total = 0\n","  for bi, d in enumerate(data_loader):\n","    ids = d[\"ids\"]\n","    mask = d[\"mask\"]\n","    token_type_ids = d[\"token_type_ids\"]\n","    targets = d[\"targets\"]\n","\n","    ids = ids.to(device, dtype=torch.long)\n","    mask = mask.to(device, dtype=torch.long)\n","    token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","    targets = targets.to(device, dtype=torch.long)\n","\n","    outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n","\n","    _, predicted = torch.max(outputs, 1)\n","    total = total + targets.size(0)\n","    corrects = corrects + (predicted==targets).sum().item()\n","\n","    print(f\"bi: {bi}\\tPredicted: {predicted}\\tTargets: {targets}\")\n","\n","  accuracy = corrects / total * 100\n","  f1 = open('/content/drive/My Drive/SentiHood/Bert-pair/QA-M/Models/' + 'accuracy.txt', 'a+')\n","  temp = f\"Corrects: {corrects}\\tTotal: {total}\\tAccuracy: {accuracy}\\n\"\n","  f1.write(temp)\n","  f1.close()\n","\n","  return accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M83SYeDt0ogD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":161},"outputId":"8a93ad81-7d6a-48a5-ce7c-d2a5dd50a470"},"source":["def run():\n","  \"\"\"\n","  This function defines hyperparameters, model and optimizer, loads required\n","  datasets and initiate the training and validation procedures.\n","  \"\"\"\n","\n","  TRAIN_MAX_LEN = 160\n","  VALID_MAX_LEN = 160\n","  TRAIN_BATCH_SIZE = 16\n","  VALID_BATCH_SIZE = 16\n","  EPOCHS = 10\n","  BERT_MODEL = 'bert-base-uncased'\n","  LEARNING_RATE = 3e-5\n","\n","  locations = ['LOCATION1', 'LOCATION2']\n","  aspects = ['dining', 'general', 'green-nature', 'live', 'multicultural', 'nightlife', 'price', 'quiet', 'safety','shopping', 'touristy', 'transit-location']\n","\n","  training_set_path = '/content/drive/My Drive/SentiHood/Bert-pair/QA-M/Datasets/training_set.csv'\n","  validation_set_path = '/content/drive/My Drive/SentiHood/Bert-pair/QA-M/Datasets/validation_set.csv'\n","\n","  df_train = pd.read_csv(training_set_path)\n","  df_valid = pd.read_csv(validation_set_path)\n","  sentiment_mapping = {\n","      'Positive': 0,\n","      'Negative': 1,\n","      'None': 2\n","  }\n","  df_train['sentiment'] = df_train['sentiment'].map(sentiment_mapping)\n","  df_valid['sentiment'] = df_valid['sentiment'].map(sentiment_mapping)\n","  df_train = df_train.reset_index(drop=True)\n","  df_valid = df_valid.reset_index(drop=True)\n","\n","  tokenizer = transformers.BertTokenizer.from_pretrained(BERT_MODEL)\n","\n","  train_dataset = SentiHood(\n","      text = df_train['text'].values,\n","      auxiliary_sentence = df_train['auxiliary_sentence'],\n","      targets = df_train['sentiment'].values,\n","      tokenizer = tokenizer,\n","      max_len = TRAIN_MAX_LEN\n","  )\n","  print(f\"Training Set: {len(train_dataset)}\")\n","\n","  # Custom sampler to compensate class imbalance in the dataset\n","  # ============================================================================\n","  class_counts = []\n","  for i in range(3):\n","    class_counts.append(df_train[df_train['sentiment']==i].shape[0])\n","  print(f\"Class Counts: {class_counts}\")\n","  \n","  num_samples = sum(class_counts)\n","  labels = df_train['sentiment'].values\n","\n","  class_weights = []\n","  for i in range(len(class_counts)):\n","    if class_counts[i] != 0:\n","      class_weights.append(num_samples/class_counts[i])\n","    else:\n","      class_weights.append(0)\n","\n","  weights = [class_weights[labels[i]] for i in range(int(num_samples))]\n","  sampler = torch.utils.data.sampler.WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))\n","  # ============================================================================\n","\n","  train_data_loader = torch.utils.data.DataLoader(\n","      train_dataset,\n","      batch_size = TRAIN_BATCH_SIZE,\n","      shuffle = False,\n","      sampler = sampler\n","  )\n","\n","  valid_dataset = SentiHood(\n","      text = df_valid['text'].values,\n","      auxiliary_sentence = df_train['auxiliary_sentence'],\n","      targets = df_valid['sentiment'].values,\n","      tokenizer = tokenizer,\n","      max_len = VALID_MAX_LEN\n","  )\n","  print(f\"Validation Set: {len(valid_dataset)}\")\n","\n","  valid_data_loader = torch.utils.data.DataLoader(\n","      valid_dataset,\n","      batch_size = VALID_BATCH_SIZE,\n","      shuffle = False\n","  )\n","\n","  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","  print(f\"Device: {device}\")\n","\n","  model = SentimentClassifier(BERT_MODEL)\n","  model = model.to(device)\n","\n","  num_train_steps = int(len(train_dataset) / TRAIN_BATCH_SIZE * EPOCHS)\n","  optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n","\n","  scheduler = lr_scheduler.StepLR(\n","      optimizer,\n","      step_size = 1,\n","      gamma = 0.8\n","  )\n","\n","  for epoch in range(EPOCHS):\n","    train_loop_function(data_loader=train_data_loader, model=model, optimizer=optimizer, device=device)\n","    accuracy = eval_loop_function(data_loader=valid_data_loader, model=model, device=device)\n","\n","    print(f\"\\nEpoch = {epoch}\\tAccuracy Score = {accuracy}\")\n","    print(f\"Learning Rate = {scheduler.get_lr()[0]}\\n\")\n","\n","    scheduler.step()\n","\n","    torch.save(model, '/content/drive/My Drive/SentiHood/Bert-pair/QA-M/Models/' + str(epoch) + '.bin')\n","\n","if __name__ == \"__main__\":\n","  run()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training Set: 45024\n","Class Counts: [2474, 921, 41629]\n","Validation Set: 11244\n","Device: cuda:0\n","Batch index = 10\tLoss = 1.2356454968452453\n","Batch index = 20\tLoss = 1.0821765899658202\n","Batch index = 30\tLoss = 1.10853910446167\n","Batch index = 40\tLoss = 1.053558784723282\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ksw8gx2tq5fz","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}