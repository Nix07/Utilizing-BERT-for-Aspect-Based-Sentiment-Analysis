{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"name":"Data Pre-processing New.ipynb","provenance":[],"collapsed_sections":["HJ-_nYnyLlIi","WksSYv5sLUwo","WnJ50OBTLUxi","36eRzoPzJItx","EgspdpzpLqz1","2j_r2Yyrjsrh","mg1UnHCjL2rt","b3ww6_uSjkSS","zn_TM8ko8XCN","RuDtixFVf009","RSQgnbYFf-FS","Aec1zSLuouce","UoAP9mohklbd","R6Pw-b34VUPR","hmZTMxDvVgQl","3QByiCX5eX9V","06Jy8JLKXM7c","hY6d1L1kUljF","5Q1LGSd4UqgK","l5Afl7pwXHGq"]}},"cells":[{"cell_type":"code","metadata":{"id":"oY79GQATNRqj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":127},"executionInfo":{"status":"ok","timestamp":1596340719474,"user_tz":-330,"elapsed":27504,"user":{"displayName":"NIKHIL PRAKASH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE64-zyQNQSjSixENWJyhGqao6rzutp-TYuXNn2Q=s64","userId":"07464489639094286985"}},"outputId":"04aee337-bd2e-45c3-93f4-95e2cd27e765"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qf4Bv1DJLUwY","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596340721427,"user_tz":-330,"elapsed":1796,"user":{"displayName":"NIKHIL PRAKASH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE64-zyQNQSjSixENWJyhGqao6rzutp-TYuXNn2Q=s64","userId":"07464489639094286985"}}},"source":["import json\n","from tqdm import tqdm\n","import pandas as pd"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HJ-_nYnyLlIi","colab_type":"text"},"source":["# Preparing Datasets for BERT-single\n","\n","For BERT-single, each `location-pair` will have a different 3-class classification model, thus we need to construct a different training and validation set for each `location-pair`. "]},{"cell_type":"markdown","metadata":{"id":"WksSYv5sLUwo","colab_type":"text"},"source":["## Preparing Training Sets"]},{"cell_type":"code","metadata":{"id":"LpNb-FQHLUwq","colab_type":"code","colab":{}},"source":["with open('/content/drive/My Drive/SentiHood/SentiHood Dataset/sentihood-train.json', 'r') as fp:\n","    training_set = json.load(fp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WDcOwcmTLUxZ","colab_type":"code","colab":{},"outputId":"e2379feb-6398-4e6b-8628-b48c74eb4577"},"source":["locations = ['LOCATION1', 'LOCATION2']\n","aspects = ['dining', 'general', 'green-nature', 'live', 'multicultural', 'nightlife', 'price', 'quiet', 'safety', 'shopping', 'touristy', 'transit-location']\n","\n","for location in locations:\n","    for aspect in aspects:\n","        df = pd.DataFrame({'id': [], 'text': [], 'sentiment': []})\n","        \n","        ii = 0\n","        for each_example in training_set:\n","            id = str(int(each_example['id']))\n","            text = each_example['text'].strip()\n","            \n","            # If `location` is present in the text, only then iterate over the  \n","            # list of opinions to find suitable `location-aspect` datapoints.\n","\n","            if location in text:\n","                aspect_found = False\n","                \n","                for opinion in each_example['opinions']:\n","                    # Checking if the current example contains a sentiment\n","                    # related to `location-aspect`\n","                    \n","                    if opinion['target_entity'] == location and opinion['aspect'] == aspect:\n","                        df.loc[ii] = [id, text, opinion['sentiment']]\n","                        aspect_found = True\n","                        ii += 1\n","                        break\n","                \n","                # If no sentiment is found for `location-asppect` in current \n","                # example, then add a datapoint with None.\n","                \n","                if not aspect_found:\n","                    df.loc[ii] = [id, text, 'None']\n","                    ii += 1\n","\n","        df.to_csv('/content/drive/My Drive/SentiHood/Bert-single/TrainingData/' + str(location) + str(aspect) + '.csv', index=False)\n","        print(f\"{location}{aspect} DONE!\\tLength = {ii}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["LOCATION1dining DONE!\tLength = 2977\n","LOCATION1general DONE!\tLength = 2977\n","LOCATION1green-nature DONE!\tLength = 2977\n","LOCATION1live DONE!\tLength = 2977\n","LOCATION1multicultural DONE!\tLength = 2977\n","LOCATION1nightlife DONE!\tLength = 2977\n","LOCATION1price DONE!\tLength = 2977\n","LOCATION1quiet DONE!\tLength = 2977\n","LOCATION1safety DONE!\tLength = 2977\n","LOCATION1shopping DONE!\tLength = 2977\n","LOCATION1touristy DONE!\tLength = 2977\n","LOCATION1transit-location DONE!\tLength = 2977\n","LOCATION2dining DONE!\tLength = 775\n","LOCATION2general DONE!\tLength = 775\n","LOCATION2green-nature DONE!\tLength = 775\n","LOCATION2live DONE!\tLength = 775\n","LOCATION2multicultural DONE!\tLength = 775\n","LOCATION2nightlife DONE!\tLength = 775\n","LOCATION2price DONE!\tLength = 775\n","LOCATION2quiet DONE!\tLength = 775\n","LOCATION2safety DONE!\tLength = 775\n","LOCATION2shopping DONE!\tLength = 775\n","LOCATION2touristy DONE!\tLength = 775\n","LOCATION2transit-location DONE!\tLength = 775\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WnJ50OBTLUxi","colab_type":"text"},"source":["## Preparing Validation Sets"]},{"cell_type":"code","metadata":{"id":"CrXWA9PNLUxk","colab_type":"code","colab":{}},"source":["with open('/content/drive/My Drive/SentiHood/SentiHood Dataset/sentihood-dev.json', 'r') as fp:\n","    validation_set = json.load(fp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EKoTqGstLUx_","colab_type":"code","colab":{},"outputId":"be897877-eb9c-4019-bf80-7cd64277e0b8"},"source":["locations = ['LOCATION1', 'LOCATION2']\n","aspects = ['dining', 'general', 'green-nature', 'live', 'multicultural', 'nightlife', 'price', 'quiet', 'safety', 'shopping', 'touristy', 'transit-location']\n","\n","for location in locations:\n","    for aspect in aspects:\n","        df = pd.DataFrame({'id': [], 'text': [], 'sentiment': []})\n","        ii = 0\n","        for each_example in validation_set:\n","            id = str(int(each_example['id']))\n","            text = each_example['text'].strip()\n","            \n","            # If `location` is present in the text, only then iterate over the  \n","            # list of opinions to find suitable `location-aspect` datapoints.\n","\n","            if location in text:\n","                aspect_found = False\n","                for opinion in each_example['opinions']:\n","                    # Checking if the current example contains a sentiment\n","                    # related to `location-aspect`\n","\n","                    if opinion['target_entity'] == location and opinion['aspect'] == aspect:\n","                        df.loc[ii] = [id, text, opinion['sentiment']]\n","                        aspect_found = True\n","                        ii += 1\n","                        break\n","                \n","                # If no sentiment is found for `location-asppect` in current \n","                # example, then add a datapoint with None.\n","\n","                if not aspect_found:\n","                    df.loc[ii] = [id, text, 'None']\n","                    ii += 1\n","\n","        df.to_csv('/content/drive/My Drive/SentiHood/Bert-single/ValidationData/' + str(location) + str(aspect) + '.csv', index=False)\n","        print(f\"{location}{aspect} DONE!\\tLength = {ii}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["LOCATION1dining DONE!\tLength = 747\n","LOCATION1general DONE!\tLength = 747\n","LOCATION1green-nature DONE!\tLength = 747\n","LOCATION1live DONE!\tLength = 747\n","LOCATION1multicultural DONE!\tLength = 747\n","LOCATION1nightlife DONE!\tLength = 747\n","LOCATION1price DONE!\tLength = 747\n","LOCATION1quiet DONE!\tLength = 747\n","LOCATION1safety DONE!\tLength = 747\n","LOCATION1shopping DONE!\tLength = 747\n","LOCATION1touristy DONE!\tLength = 747\n","LOCATION1transit-location DONE!\tLength = 747\n","LOCATION2dining DONE!\tLength = 190\n","LOCATION2general DONE!\tLength = 190\n","LOCATION2green-nature DONE!\tLength = 190\n","LOCATION2live DONE!\tLength = 190\n","LOCATION2multicultural DONE!\tLength = 190\n","LOCATION2nightlife DONE!\tLength = 190\n","LOCATION2price DONE!\tLength = 190\n","LOCATION2quiet DONE!\tLength = 190\n","LOCATION2safety DONE!\tLength = 190\n","LOCATION2shopping DONE!\tLength = 190\n","LOCATION2touristy DONE!\tLength = 190\n","LOCATION2transit-location DONE!\tLength = 190\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"36eRzoPzJItx","colab_type":"text"},"source":["## Preparing Testing Sets"]},{"cell_type":"code","metadata":{"id":"U6_j4QJmJT0t","colab_type":"code","colab":{}},"source":["with open('/content/drive/My Drive/SentiHood/SentiHood Dataset/sentihood-test.json', 'r') as fp:\n","    testing_set = json.load(fp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2utSpvT1JaxV","colab_type":"code","colab":{}},"source":["locations = ['LOCATION1', 'LOCATION2']\n","aspects = ['dining', 'general', 'green-nature', 'live', 'multicultural', 'nightlife', 'price', 'quiet', 'safety', 'shopping', 'touristy', 'transit-location']\n","\n","for location in locations:\n","    for aspect in aspects:\n","        df = pd.DataFrame({'id': [], 'text': [], 'sentiment': []})\n","        ii = 0\n","        for each_example in testing_set:\n","            id = str(int(each_example['id']))\n","            text = each_example['text'].strip()\n","            \n","            # If `location` is present in the text, only then iterate over the  \n","            # list of opinions to find suitable `location-aspect` datapoints.\n","\n","            if location in text:\n","                aspect_found = False\n","                for opinion in each_example['opinions']:\n","                    # Checking if the current example contains a sentiment\n","                    # related to `location-aspect`\n","\n","                    if opinion['target_entity'] == location and opinion['aspect'] == aspect:\n","                        df.loc[ii] = [id, text, opinion['sentiment']]\n","                        aspect_found = True\n","                        ii += 1\n","                        break\n","                \n","                # If no sentiment is found for `location-asppect` in current \n","                # example, then add a datapoint with None.\n","\n","                if not aspect_found:\n","                    df.loc[ii] = [id, text, 'None']\n","                    ii += 1\n","\n","        df.to_csv('/content/drive/My Drive/SentiHood/Bert-single/TestingData/' + str(location) + str(aspect) + '.csv', index=False)\n","        print(f\"{location}{aspect} DONE!\\tLength = {ii}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EgspdpzpLqz1","colab_type":"text"},"source":["# Preparing Datasets for BERT-pair\n","\n","For all the BERT-pair models, only a single training, validation and testing set need to constructed."]},{"cell_type":"markdown","metadata":{"id":"2j_r2Yyrjsrh","colab_type":"text"},"source":["## Datasets for QA-M\n","\n","For QA-M, an auxiliary sentence in the following form will be constructed for each `location-aspect`.\n","\n","*what do you think about `aspect` of `location`?*"]},{"cell_type":"markdown","metadata":{"id":"mg1UnHCjL2rt","colab_type":"text"},"source":["### Training set for BERT-pair QA-M"]},{"cell_type":"code","metadata":{"id":"fJMO6XbMLwxG","colab_type":"code","colab":{}},"source":["with open('/content/drive/My Drive/SentiHood/SentiHood Dataset/sentihood-train.json', 'r') as fp:\n","    training_set = json.load(fp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d_DjXpAePMs9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1595988411038,"user_tz":-330,"elapsed":192237,"user":{"displayName":"Nikhil Prakash","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh2n3ooINSV1n1kZoSymqxX7GhkFUxPi4tw-pohqA=s64","userId":"14280933032296268011"}},"outputId":"590e9335-bbbe-463d-e911-ce7b505ce03c"},"source":["locations = ['LOCATION1', 'LOCATION2']\n","aspects = ['dining', 'general', 'green-nature', 'live', 'multicultural', 'nightlife', 'price', 'quiet', 'safety', 'shopping', 'touristy', 'transit-location']\n","\n","df = pd.DataFrame({'id': [], 'text': [], 'auxiliary_sentence': [], 'sentiment': []})\n","ii = 0\n","\n","for location in locations:\n","  for each_example in tqdm(training_set, ncols=80):\n","    id = str(int(each_example['id']))\n","    text = each_example['text'].strip()\n","\n","    # If `location` is present in the text, only then iterate over the  \n","    # list of opinions to find suitable `location-aspect` datapoints.\n","\n","    if location in text:\n","      # current_opinion_aspects: Is a dictionary containing all aspects and  \n","      # their corresponding sentiment present in the current opinion.\n","      current_opinion_aspects = {}\n","      \n","      for opinion in each_example['opinions']:\n","        if opinion['target_entity'] == location:\n","          current_opinion_aspects[opinion['aspect']] = opinion['sentiment']\n","      \n","      for aspect in aspects:\n","        auxiliary_sentence = f'what do you think of the {aspect} of {location}?' \n","        \n","        if aspect in current_opinion_aspects.keys():\n","          df.loc[ii] = [id, text, auxiliary_sentence, current_opinion_aspects[aspect]]\n","        else:\n","          df.loc[ii] = [id, text, auxiliary_sentence, 'None']\n","        ii += 1\n","          "],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 2977/2977 [02:23<00:00, 20.78it/s]\n","100%|██████████| 2977/2977 [00:48<00:00, 61.58it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"WRyqc4nLVbwJ","colab_type":"code","colab":{}},"source":["# df.to_csv('/content/drive/My Drive/SentiHood/Bert-pair/QA-M/training_set.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b3ww6_uSjkSS","colab_type":"text"},"source":["### Validation set for BERT-pair QA-M"]},{"cell_type":"code","metadata":{"id":"XQx2iZyjgetn","colab_type":"code","colab":{}},"source":["with open('/content/drive/My Drive/SentiHood/SentiHood Dataset/sentihood-dev.json', 'r') as fp:\n","    validation_set = json.load(fp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QjGyZ-Rmj3hi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1595993003637,"user_tz":-330,"elapsed":38813,"user":{"displayName":"Nikhil Prakash","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh2n3ooINSV1n1kZoSymqxX7GhkFUxPi4tw-pohqA=s64","userId":"14280933032296268011"}},"outputId":"b642ec8d-f19e-41ef-d8ac-9218e42673af"},"source":["locations = ['LOCATION1', 'LOCATION2']\n","aspects = ['dining', 'general', 'green-nature', 'live', 'multicultural', 'nightlife', 'price', 'quiet', 'safety', 'shopping', 'touristy', 'transit-location']\n","\n","df = pd.DataFrame({'id': [], 'text': [], 'auxiliary_sentence': [], 'sentiment': []})\n","ii = 0\n","\n","for location in locations:\n","  for each_example in tqdm(validation_set, ncols=80):\n","    id = str(int(each_example['id']))\n","    text = each_example['text'].strip()\n","\n","    # If `location` is present in the text, only then iterate over the  \n","    # list of opinions to find suitable `location-aspect` datapoints.\n","\n","    if location in text:\n","      # current_opinion_aspects: Is a dictionary containing all aspects and  \n","      # their corresponding sentiment present in the current opinion.\n","      current_opinion_aspects = {}\n","      \n","      for opinion in each_example['opinions']:\n","        if opinion['target_entity'] == location:\n","          current_opinion_aspects[opinion['aspect']] = opinion['sentiment']\n","      \n","      for aspect in aspects:\n","        auxiliary_sentence = f'what do you think of the {aspect} of {location}?' \n","        if aspect in current_opinion_aspects.keys():\n","          df.loc[ii] = [id, text, auxiliary_sentence, current_opinion_aspects[aspect]]\n","        else:\n","          df.loc[ii] = [id, text, auxiliary_sentence, 'None']\n","        ii += 1\n","          "],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|█████████████████████████████████████████| 747/747 [00:29<00:00, 24.95it/s]\n","100%|█████████████████████████████████████████| 747/747 [00:08<00:00, 92.77it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"lX_IeSNeltjG","colab_type":"code","colab":{}},"source":["# df.to_csv('/content/drive/My Drive/SentiHood/Bert-pair/QA-M/validation_set.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zn_TM8ko8XCN","colab_type":"text"},"source":["### Testing set for BERT-pair QA-M"]},{"cell_type":"code","metadata":{"id":"nMX2sS1dmW95","colab_type":"code","colab":{}},"source":["with open('/content/drive/My Drive/SentiHood/SentiHood Dataset/sentihood-test.json', 'r') as fp:\n","    testing_set = json.load(fp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9y6TsLTw8rO6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1596032558268,"user_tz":-330,"elapsed":81928,"user":{"displayName":"NIKHIL PRAKASH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE64-zyQNQSjSixENWJyhGqao6rzutp-TYuXNn2Q=s64","userId":"07464489639094286985"}},"outputId":"69a8a58f-4960-42dd-bb75-4b87619ec996"},"source":["locations = ['LOCATION1', 'LOCATION2']\n","aspects = ['dining', 'general', 'green-nature', 'live', 'multicultural', 'nightlife', 'price', 'quiet', 'safety', 'shopping', 'touristy', 'transit-location']\n","\n","df = pd.DataFrame({'id': [], 'text': [], 'auxiliary_sentence': [], 'sentiment': []})\n","ii = 0\n","\n","for location in locations:\n","  for each_example in tqdm(testing_set, ncols=80):\n","    id = str(int(each_example['id']))\n","    text = each_example['text'].strip()\n","\n","    # If `location` is present in the text, only then iterate over the  \n","    # list of opinions to find suitable `location-aspect` datapoints.\n","    \n","    if location in text:\n","      # current_opinion_aspects: Is a dictionary containing all aspects and  \n","      # their corresponding sentiment present in the current opinion.\n","\n","      current_opinion_aspects = {}\n","      for opinion in each_example['opinions']:\n","        if opinion['target_entity'] == location:\n","          current_opinion_aspects[opinion['aspect']] = opinion['sentiment']\n","      \n","      for aspect in aspects:\n","        auxiliary_sentence = f'what do you think of the {aspect} of {location}?' \n","        if aspect in current_opinion_aspects.keys():\n","          df.loc[ii] = [id, text, auxiliary_sentence, current_opinion_aspects[aspect]]\n","        else:\n","          df.loc[ii] = [id, text, auxiliary_sentence, 'None']\n","        ii += 1\n","          "],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|███████████████████████████████████████| 1491/1491 [01:02<00:00, 23.71it/s]\n","100%|███████████████████████████████████████| 1491/1491 [00:17<00:00, 83.23it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"en2g97-49L4r","colab_type":"code","colab":{}},"source":["# df.to_csv('/content/drive/My Drive/SentiHood/Bert-pair/QA-M/testing_set.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RuDtixFVf009","colab_type":"text"},"source":["## Datasets for NLI-M\n","\n","For NLI-M, an auxiliary pseudo-sentence is the following form will be constructed for each `location-aspect`.\n","\n","*`location` - `aspect`* where `location` will be reformed as `location - 1` and `location - 2`."]},{"cell_type":"markdown","metadata":{"id":"RSQgnbYFf-FS","colab_type":"text"},"source":["### Training set for BERT-pair NLI-M"]},{"cell_type":"code","metadata":{"id":"GBHmjLPaf6tN","colab_type":"code","colab":{}},"source":["with open('/content/drive/My Drive/SentiHood/SentiHood Dataset/sentihood-train.json', 'r') as fp:\n","    training_set = json.load(fp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P02JxTGJgFcL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1596043847693,"user_tz":-330,"elapsed":237517,"user":{"displayName":"NIKHIL PRAKASH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE64-zyQNQSjSixENWJyhGqao6rzutp-TYuXNn2Q=s64","userId":"07464489639094286985"}},"outputId":"4d64bc67-9c48-4c97-cb18-efc1a403b284"},"source":["locations = ['LOCATION1', 'LOCATION2']\n","aspects = ['dining', 'general', 'green-nature', 'live', 'multicultural', 'nightlife', 'price', 'quiet', 'safety', 'shopping', 'touristy', 'transit-location']\n","\n","df = pd.DataFrame({'id': [], 'text': [], 'auxiliary_sentence': [], 'sentiment': []})\n","ii = 0\n","\n","count_location = 1\n","for location in locations:\n","  for each_example in tqdm(training_set, ncols=80):\n","    id = str(int(each_example['id']))\n","    text = each_example['text'].strip()\n","\n","    # If `location` is present in the text, only then iterate over the  \n","    # list of opinions to find suitable `location-aspect` datapoints.\n","\n","    if location in text:\n","      # current_opinion_aspects: Is a dictionary containing all aspects and  \n","      # their corresponding sentiment present in the current opinion.\n","      current_opinion_aspects = {}\n","\n","      for opinion in each_example['opinions']:\n","        if opinion['target_entity'] == location:\n","          current_opinion_aspects[opinion['aspect']] = opinion['sentiment']\n","      \n","      for aspect in aspects:\n","        auxiliary_sentence = f\"location - {str(count_location)} - {aspect}\"\n","        text = text.replace(location, 'location - ' + str(count_location))\n","        \n","        if aspect in current_opinion_aspects.keys():\n","          df.loc[ii] = [id, text, auxiliary_sentence, current_opinion_aspects[aspect]]\n","        else:\n","          df.loc[ii] = [id, text, auxiliary_sentence, 'None']\n","        ii += 1\n","    \n","  count_location += 1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|███████████████████████████████████████| 2977/2977 [02:53<00:00, 17.20it/s]\n","100%|███████████████████████████████████████| 2977/2977 [01:03<00:00, 46.70it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"oJtYHDdDiDmA","colab_type":"code","colab":{}},"source":["# df.to_csv('/content/drive/My Drive/SentiHood/Bert-pair/NLI-M/Datasets/training_set.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Aec1zSLuouce","colab_type":"text"},"source":["### Validation set for BERT-pair NLI-M"]},{"cell_type":"code","metadata":{"id":"eqHXsnYbm79o","colab_type":"code","colab":{}},"source":["with open('/content/drive/My Drive/SentiHood/SentiHood Dataset/sentihood-dev.json', 'r') as fp:\n","    validation_set = json.load(fp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tT_DAuuco25i","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1596044124071,"user_tz":-330,"elapsed":43215,"user":{"displayName":"NIKHIL PRAKASH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE64-zyQNQSjSixENWJyhGqao6rzutp-TYuXNn2Q=s64","userId":"07464489639094286985"}},"outputId":"eb49a477-e485-4df5-a720-4fb22fb1e6e1"},"source":["locations = ['LOCATION1', 'LOCATION2']\n","aspects = ['dining', 'general', 'green-nature', 'live', 'multicultural', 'nightlife', 'price', 'quiet', 'safety', 'shopping', 'touristy', 'transit-location']\n","\n","df = pd.DataFrame({'id': [], 'text': [], 'auxiliary_sentence': [], 'sentiment': []})\n","ii = 0\n","\n","count_location = 1\n","for location in locations:\n","  for each_example in tqdm(validation_set, ncols=80):\n","    id = str(int(each_example['id']))\n","    text = each_example['text'].strip()\n","\n","    # If `location` is present in the text, only then iterate over the  \n","    # list of opinions to find suitable `location-aspect` datapoints.\n","\n","    if location in text:\n","      # current_opinion_aspects: Is a dictionary containing all aspects and  \n","      # their corresponding sentiment present in the current opinion.\n","      current_opinion_aspects = {}\n","\n","      for opinion in each_example['opinions']:\n","        if opinion['target_entity'] == location:\n","          current_opinion_aspects[opinion['aspect']] = opinion['sentiment']\n","      \n","      for aspect in aspects:\n","        auxiliary_sentence = f\"location - {str(count_location)} - {aspect}\"\n","        text = text.replace(location, 'location - ' + str(count_location))\n","        \n","        if aspect in current_opinion_aspects.keys():\n","          df.loc[ii] = [id, text, auxiliary_sentence, current_opinion_aspects[aspect]]\n","        else:\n","          df.loc[ii] = [id, text, auxiliary_sentence, 'None']\n","        ii += 1\n","    \n","  count_location += 1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|█████████████████████████████████████████| 747/747 [00:33<00:00, 22.58it/s]\n","100%|█████████████████████████████████████████| 747/747 [00:09<00:00, 81.97it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"8KBLuKtPo_Y2","colab_type":"code","colab":{}},"source":["# df.to_csv('/content/drive/My Drive/SentiHood/Bert-pair/NLI-M/Datasets/validation_set.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UoAP9mohklbd","colab_type":"text"},"source":["### Testing set for BERT-pair NLI-M "]},{"cell_type":"code","metadata":{"id":"WONz_7CwpLad","colab_type":"code","colab":{}},"source":["with open('/content/drive/My Drive/SentiHood/SentiHood Dataset/sentihood-test.json', 'r') as fp:\n","    testing_set = json.load(fp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"55YuOxfDkwm-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1596077154014,"user_tz":-330,"elapsed":78524,"user":{"displayName":"NIKHIL PRAKASH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE64-zyQNQSjSixENWJyhGqao6rzutp-TYuXNn2Q=s64","userId":"07464489639094286985"}},"outputId":"5884dbf5-7676-4d4d-bbba-d5abe8473f85"},"source":["locations = ['LOCATION1', 'LOCATION2']\n","aspects = ['dining', 'general', 'green-nature', 'live', 'multicultural', 'nightlife', 'price', 'quiet', 'safety', 'shopping', 'touristy', 'transit-location']\n","\n","df = pd.DataFrame({'id': [], 'text': [], 'auxiliary_sentence': [], 'sentiment': []})\n","ii = 0\n","\n","count_location = 1\n","for location in locations:\n","  for each_example in tqdm(testing_set, ncols=80):\n","    id = str(int(each_example['id']))\n","    text = each_example['text'].strip()\n","\n","    # If `location` is present in the text, only then iterate over the  \n","    # list of opinions to find suitable `location-aspect` datapoints.\n","\n","    if location in text:\n","      # current_opinion_aspects: Is a dictionary containing all aspects and  \n","      # their corresponding sentiment present in the current opinion.\n","      current_opinion_aspects = {}\n","\n","      for opinion in each_example['opinions']:\n","        if opinion['target_entity'] == location:\n","          current_opinion_aspects[opinion['aspect']] = opinion['sentiment']\n","      \n","      for aspect in aspects:\n","        auxiliary_sentence = f\"location - {str(count_location)} - {aspect}\"\n","        text = text.replace(location, 'location - ' + str(count_location))\n","        \n","        if aspect in current_opinion_aspects.keys():\n","          df.loc[ii] = [id, text, auxiliary_sentence, current_opinion_aspects[aspect]]\n","        else:\n","          df.loc[ii] = [id, text, auxiliary_sentence, 'None']\n","        ii += 1\n","    \n","  count_location += 1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|███████████████████████████████████████| 1491/1491 [01:00<00:00, 24.82it/s]\n","100%|███████████████████████████████████████| 1491/1491 [00:17<00:00, 85.07it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"vax7je3Km2uQ","colab_type":"code","colab":{}},"source":["# df.to_csv('/content/drive/My Drive/SentiHood/Bert-pair/NLI-M/Datasets/testing_set.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R6Pw-b34VUPR","colab_type":"text"},"source":["## Datasets for QA-B\n","\n","For QA-B, 3 auxiliary sentences in the following form will be constructed for each `location-aspect`.\n","\n","*the polarity of the aspect `aspect` of `location` is `positive|negative|none`*."]},{"cell_type":"markdown","metadata":{"id":"hmZTMxDvVgQl","colab_type":"text"},"source":["### Training set for BERT-pair QA-B"]},{"cell_type":"code","metadata":{"id":"7uO-4GB5nP4J","colab_type":"code","colab":{}},"source":["with open('/content/drive/My Drive/SentiHood/SentiHood Dataset/sentihood-train.json', 'r') as fp:\n","    training_set = json.load(fp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xQkGGnm9VquY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1596199684532,"user_tz":-330,"elapsed":290698,"user":{"displayName":"NIKHIL PRAKASH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE64-zyQNQSjSixENWJyhGqao6rzutp-TYuXNn2Q=s64","userId":"07464489639094286985"}},"outputId":"6c70104d-0e4e-4213-e6c6-984500133335"},"source":["locations = ['LOCATION1', 'LOCATION2']\n","aspects = ['dining', 'general', 'green-nature', 'live', 'multicultural', 'nightlife', 'price', 'quiet', 'safety', 'shopping', 'touristy', 'transit-location']\n","\n","df = pd.DataFrame({'id': [], 'text': [], 'auxiliary_sentence': [], 'sentiment': []})\n","ii = 0\n","\n","count_location = 1\n","for location in locations:\n","  for each_example in tqdm(training_set, ncols=80):\n","    id = str(int(each_example['id']))\n","    text = each_example['text'].strip()\n","\n","    # If `location` is present in the text, only then iterate over the  \n","    # list of opinions to find suitable `location-aspect` datapoints.\n","\n","    if location in text:\n","      # current_opinion_aspects: Is a dictionary containing all aspects and  \n","      # their corresponding sentiment present in the current opinion.\n","\n","      current_opinion_aspects = {}\n","      for opinion in each_example['opinions']:\n","        if opinion['target_entity'] == location:\n","          current_opinion_aspects[opinion['aspect']] = opinion['sentiment']\n","      \n","      text = text.replace(location, 'location - ' + str(count_location))\n","      \n","      for aspect in aspects:\n","        aspect_found = False\n","        \n","        for polarity in ['Positive', 'Negative']:\n","          auxiliary_sentence = f\"the polarity of the aspect {aspect} of location - {str(count_location)} is {polarity}.\"\n","          \n","          if aspect in current_opinion_aspects.keys() and current_opinion_aspects[aspect] == polarity:\n","            df.loc[ii] = [id, text, auxiliary_sentence, int(1)]\n","            ii += 1\n","            aspect_found = True\n","          else:\n","            df.loc[ii] = [id, text, auxiliary_sentence, int(0)]\n","            ii += 1\n","        \n","        # If no sentiment is found for the `location-aspect`, then add the \n","        # auxiliary sentence with None to the dataset.\n","        \n","        auxiliary_sentence = f\"the polarity of the aspect {aspect} of location - {str(count_location)} is None.\"\n","        if not aspect_found:\n","          df.loc[ii] = [id, text, auxiliary_sentence, int(1)]\n","          ii += 1\n","        else:\n","          df.loc[ii] = [id, text, auxiliary_sentence, int(0)]\n","          ii += 1\n","    \n","  count_location += 1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|███████████████████████████████████████| 2977/2977 [13:51<00:00,  3.58it/s]\n","100%|███████████████████████████████████████| 2977/2977 [06:59<00:00,  7.09it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"fkMTRULiYOuH","colab_type":"code","colab":{}},"source":["df['sentiment'] = df['sentiment'].astype(int)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"83zIDzuTdD1L","colab_type":"code","colab":{}},"source":["# df.to_csv('/content/drive/My Drive/SentiHood/Bert-pair/QA-B/Datasets/training_set.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3QByiCX5eX9V","colab_type":"text"},"source":["### Validation set for BERT-pair QA-B"]},{"cell_type":"code","metadata":{"id":"iPvFDsajeJ2p","colab_type":"code","colab":{}},"source":["with open('/content/drive/My Drive/SentiHood/SentiHood Dataset/sentihood-dev.json', 'r') as fp:\n","    validation_set = json.load(fp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jMZoTRrUefo7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1596199920772,"user_tz":-330,"elapsed":150082,"user":{"displayName":"NIKHIL PRAKASH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE64-zyQNQSjSixENWJyhGqao6rzutp-TYuXNn2Q=s64","userId":"07464489639094286985"}},"outputId":"9b44e21e-18d3-41c0-f18b-917dd3b42eef"},"source":["locations = ['LOCATION1', 'LOCATION2']\n","aspects = ['dining', 'general', 'green-nature', 'live', 'multicultural', 'nightlife', 'price', 'quiet', 'safety', 'shopping', 'touristy', 'transit-location']\n","\n","df = pd.DataFrame({'id': [], 'text': [], 'auxiliary_sentence': [], 'sentiment': []})\n","ii = 0\n","\n","count_location = 1\n","for location in locations:\n","  for each_example in tqdm(validation_set, ncols=80):\n","    id = str(int(each_example['id']))\n","    text = each_example['text'].strip()\n","\n","    # If `location` is present in the text, only then iterate over the  \n","    # list of opinions to find suitable `location-aspect` datapoints.\n","    \n","    if location in text:\n","      # current_opinion_aspects: Is a dictionary containing all aspects and  \n","      # their corresponding sentiment present in the current opinion.\n","\n","      current_opinion_aspects = {}\n","      for opinion in each_example['opinions']:\n","        if opinion['target_entity'] == location:\n","          current_opinion_aspects[opinion['aspect']] = opinion['sentiment']\n","      \n","      text = text.replace(location, 'location - ' + str(count_location))\n","      \n","      for aspect in aspects:\n","        aspect_found = False\n","        \n","        for polarity in ['Positive', 'Negative']:\n","          auxiliary_sentence = f\"the polarity of the aspect {aspect} of location - {str(count_location)} is {polarity}.\"\n","          \n","          if aspect in current_opinion_aspects.keys() and current_opinion_aspects[aspect] == polarity:\n","            df.loc[ii] = [id, text, auxiliary_sentence, int(1)]\n","            ii += 1\n","            aspect_found = True\n","          else:\n","            df.loc[ii] = [id, text, auxiliary_sentence, int(0)]\n","            ii += 1\n","        \n","        # If no sentiment is found for the `location-aspect`, then add the \n","        # auxiliary sentence with None to the dataset.\n","\n","        auxiliary_sentence = f\"the polarity of the aspect {aspect} of location - {str(count_location)} is None.\"\n","        if not aspect_found:\n","          df.loc[ii] = [id, text, auxiliary_sentence, int(1)]\n","          ii += 1\n","        else:\n","          df.loc[ii] = [id, text, auxiliary_sentence, int(0)]\n","          ii += 1\n","    \n","  count_location += 1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|█████████████████████████████████████████| 747/747 [01:53<00:00,  6.60it/s]\n","100%|█████████████████████████████████████████| 747/747 [00:36<00:00, 20.61it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"-WnxHoWsDqqo","colab_type":"code","colab":{}},"source":["df['sentiment'] = df['sentiment'].astype(int)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ekpQlCDfgpW","colab_type":"code","colab":{}},"source":["# df.to_csv('/content/drive/My Drive/SentiHood/Bert-pair/QA-B/Datasets/validation_set.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"06Jy8JLKXM7c","colab_type":"text"},"source":["### Testing set for BERT-pair QA-B "]},{"cell_type":"code","metadata":{"id":"i-YUuV1gXMHP","colab_type":"code","colab":{}},"source":["with open('/content/drive/My Drive/SentiHood/SentiHood Dataset/sentihood-test.json', 'r') as fp:\n","    testing_set = json.load(fp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2hxcy5xefuRw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1596250719156,"user_tz":-330,"elapsed":426665,"user":{"displayName":"NIKHIL PRAKASH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE64-zyQNQSjSixENWJyhGqao6rzutp-TYuXNn2Q=s64","userId":"07464489639094286985"}},"outputId":"99394567-faf2-461b-c6fc-7e56467e642d"},"source":["locations = ['LOCATION1', 'LOCATION2']\n","aspects = ['dining', 'general', 'green-nature', 'live', 'multicultural', 'nightlife', 'price', 'quiet', 'safety', 'shopping', 'touristy', 'transit-location']\n","\n","df = pd.DataFrame({'id': [], 'text': [], 'auxiliary_sentence': [], 'sentiment': []})\n","ii = 0\n","\n","count_location = 1\n","for location in locations:\n","  for each_example in tqdm(testing_set, ncols=80):\n","    id = str(int(each_example['id']))\n","    text = each_example['text'].strip()\n","\n","    # If `location` is present in the text, only then iterate over the  \n","    # list of opinions to find suitable `location-aspect` datapoints.\n","\n","    if location in text:\n","      # current_opinion_aspects: Is a dictionary containing all aspects and  \n","      # their corresponding sentiment present in the current opinion.\n","\n","      current_opinion_aspects = {}\n","      for opinion in each_example['opinions']:\n","        if opinion['target_entity'] == location:\n","          current_opinion_aspects[opinion['aspect']] = opinion['sentiment']\n","      \n","      text = text.replace(location, 'location - ' + str(count_location))\n","      \n","      for aspect in aspects:\n","        aspect_found = False\n","        \n","        for polarity in ['Positive', 'Negative']:\n","          auxiliary_sentence = f\"the polarity of the aspect {aspect} of location - {str(count_location)} is {polarity}.\"\n","          \n","          if aspect in current_opinion_aspects.keys() and current_opinion_aspects[aspect] == polarity:\n","            df.loc[ii] = [id, text, auxiliary_sentence, int(1)]\n","            ii += 1\n","            aspect_found = True\n","          else:\n","            df.loc[ii] = [id, text, auxiliary_sentence, int(0)]\n","            ii += 1\n","        \n","        # If no sentiment is found for the `location-aspect`, then add the \n","        # auxiliary sentence with None to the dataset.\n","        \n","        auxiliary_sentence = f\"the polarity of the aspect {aspect} of location - {str(count_location)} is None.\"\n","        if not aspect_found:\n","          df.loc[ii] = [id, text, auxiliary_sentence, int(1)]\n","          ii += 1\n","        else:\n","          df.loc[ii] = [id, text, auxiliary_sentence, int(0)]\n","          ii += 1\n","    \n","  count_location += 1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|███████████████████████████████████████| 1491/1491 [04:55<00:00,  5.04it/s]\n","100%|███████████████████████████████████████| 1491/1491 [02:10<00:00, 11.46it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"mBUv4P5yaNEa","colab_type":"code","colab":{}},"source":["df['sentiment'] = df['sentiment'].astype(int)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gwgJ3xcMdHc3","colab_type":"code","colab":{}},"source":["# df.to_csv('/content/drive/My Drive/SentiHood/Bert-pair/QA-B/Datasets/testing_set.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hY6d1L1kUljF","colab_type":"text"},"source":["## Datasets for NLI-B\n","\n","For NLI-B, 3 auxiliary pseudo-sentences in the following form will be constructed for each `location-aspect`.\n","\n","*`location` - `aspect` - `polarity`*"]},{"cell_type":"markdown","metadata":{"id":"5Q1LGSd4UqgK","colab_type":"text"},"source":["### Training set for BERT-pair NLI-B"]},{"cell_type":"code","metadata":{"id":"VsDWHrRB9dSN","colab_type":"code","colab":{}},"source":["with open('/content/drive/My Drive/SentiHood/SentiHood Dataset/sentihood-train.json', 'r') as fp:\n","    training_set = json.load(fp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QB-fXiGIU3K0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1596292155806,"user_tz":-330,"elapsed":1252252,"user":{"displayName":"NIKHIL PRAKASH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE64-zyQNQSjSixENWJyhGqao6rzutp-TYuXNn2Q=s64","userId":"07464489639094286985"}},"outputId":"90fe0ad8-0c99-422d-8b1b-041768cde98c"},"source":["locations = ['LOCATION1', 'LOCATION2']\n","aspects = ['dining', 'general', 'green-nature', 'live', 'multicultural', 'nightlife', 'price', 'quiet', 'safety', 'shopping', 'touristy', 'transit-location']\n","\n","df = pd.DataFrame({'id': [], 'text': [], 'auxiliary_sentence': [], 'sentiment': []})\n","ii = 0\n","\n","count_location = 1\n","for location in locations:\n","  for each_example in tqdm(training_set, ncols=80):\n","    id = str(int(each_example['id']))\n","    text = each_example['text'].strip()\n","\n","    # If `location` is present in the text, only then iterate over the  \n","    # list of opinions to find suitable `location-aspect` datapoints.\n","\n","    if location in text:\n","      # current_opinion_aspects: Is a dictionary containing all aspects and  \n","      # their corresponding sentiment present in the current opinion.\n","\n","      current_opinion_aspects = {}\n","      for opinion in each_example['opinions']:\n","        if opinion['target_entity'] == location:\n","          current_opinion_aspects[opinion['aspect']] = opinion['sentiment']\n","      \n","      text = text.replace(location, 'location - ' + str(count_location))\n","      \n","      for aspect in aspects:\n","        aspect_found = False\n","        \n","        for polarity in ['Positive', 'Negative']:\n","          auxiliary_sentence = f\"location - {str(count_location)} - {aspect} - {polarity}.\"\n","          \n","          if aspect in current_opinion_aspects.keys() and current_opinion_aspects[aspect] == polarity:\n","            df.loc[ii] = [id, text, auxiliary_sentence, int(1)]\n","            ii += 1\n","            aspect_found = True\n","          else:\n","            df.loc[ii] = [id, text, auxiliary_sentence, int(0)]\n","            ii += 1\n","        \n","        # If no sentiment is found for the `location-aspect`, then add the \n","        # auxiliary sentence with None to the dataset.\n","        \n","        auxiliary_sentence = f\"location - {str(count_location)} - {aspect} - None.\"\n","        if not aspect_found:\n","          df.loc[ii] = [id, text, auxiliary_sentence, int(1)]\n","          ii += 1\n","        else:\n","          df.loc[ii] = [id, text, auxiliary_sentence, int(0)]\n","          ii += 1\n","    \n","  count_location += 1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|███████████████████████████████████████| 2977/2977 [14:10<00:00,  3.50it/s]\n","100%|███████████████████████████████████████| 2977/2977 [06:40<00:00,  7.44it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"dyBGvaABVbJL","colab_type":"code","colab":{}},"source":["df['sentiment'] =  df['sentiment'].astype(int)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vFyYLzM5Vi8-","colab_type":"code","colab":{}},"source":["# df.to_csv('/content/drive/My Drive/SentiHood/Bert-pair/NLI-B/Datasets/training_set.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l5Afl7pwXHGq","colab_type":"text"},"source":["### Validation set for BERT-pair NLI-B"]},{"cell_type":"code","metadata":{"id":"1tzn7aQFXLAu","colab_type":"code","colab":{}},"source":["with open('/content/drive/My Drive/SentiHood/SentiHood Dataset/sentihood-dev.json', 'r') as fp:\n","    validation_set = json.load(fp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GWndyG8IXQAX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1596275547366,"user_tz":-330,"elapsed":160068,"user":{"displayName":"NIKHIL PRAKASH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE64-zyQNQSjSixENWJyhGqao6rzutp-TYuXNn2Q=s64","userId":"07464489639094286985"}},"outputId":"0088386e-a5b0-4ed4-a50f-7db25112d893"},"source":["locations = ['LOCATION1', 'LOCATION2']\n","aspects = ['dining', 'general', 'green-nature', 'live', 'multicultural', 'nightlife', 'price', 'quiet', 'safety', 'shopping', 'touristy', 'transit-location']\n","\n","df = pd.DataFrame({'id': [], 'text': [], 'auxiliary_sentence': [], 'sentiment': []})\n","ii = 0\n","\n","count_location = 1\n","for location in locations:\n","  for each_example in tqdm(validation_set, ncols=80):\n","    id = str(int(each_example['id']))\n","    text = each_example['text'].strip()\n","\n","    # If `location` is present in the text, only then iterate over the  \n","    # list of opinions to find suitable `location-aspect` datapoints.\n","\n","    if location in text:\n","      # current_opinion_aspects: Is a dictionary containing all aspects and  \n","      # their corresponding sentiment present in the current opinion.\n","\n","      current_opinion_aspects = {}\n","      for opinion in each_example['opinions']:\n","        if opinion['target_entity'] == location:\n","          current_opinion_aspects[opinion['aspect']] = opinion['sentiment']\n","      \n","      text = text.replace(location, 'location - ' + str(count_location))\n","      \n","      for aspect in aspects:\n","        aspect_found = False\n","        \n","        for polarity in ['Positive', 'Negative']:\n","          auxiliary_sentence = f\"location - {str(count_location)} - {aspect} - {polarity}.\"\n","          \n","          if aspect in current_opinion_aspects.keys() and current_opinion_aspects[aspect] == polarity:\n","            df.loc[ii] = [id, text, auxiliary_sentence, int(1)]\n","            ii += 1\n","            aspect_found = True\n","          else:\n","            df.loc[ii] = [id, text, auxiliary_sentence, int(0)]\n","            ii += 1\n","        \n","        # If no sentiment is found for the `location-aspect`, then add the \n","        # auxiliary sentence with None to the dataset.\n","        \n","        auxiliary_sentence = f\"location - {str(count_location)} - {aspect} - None.\"\n","        if not aspect_found:\n","          df.loc[ii] = [id, text, auxiliary_sentence, int(1)]\n","          ii += 1\n","        else:\n","          df.loc[ii] = [id, text, auxiliary_sentence, int(0)]\n","          ii += 1\n","    \n","  count_location += 1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|█████████████████████████████████████████| 747/747 [01:58<00:00,  6.30it/s]\n","100%|█████████████████████████████████████████| 747/747 [00:40<00:00, 18.61it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"fXK0Yt-fXpQm","colab_type":"code","colab":{}},"source":["df['sentiment'] =  df['sentiment'].astype(int)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VZ1WMrBcXpyQ","colab_type":"code","colab":{}},"source":["# df.to_csv('/content/drive/My Drive/SentiHood/Bert-pair/NLI-B/Datasets/validation_set.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hZss4EK7UPkD","colab_type":"text"},"source":["### Testing set for BERT-pair NLI-B"]},{"cell_type":"code","metadata":{"id":"oRh7bxW8X5sh","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596340732601,"user_tz":-330,"elapsed":2232,"user":{"displayName":"NIKHIL PRAKASH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE64-zyQNQSjSixENWJyhGqao6rzutp-TYuXNn2Q=s64","userId":"07464489639094286985"}}},"source":["with open('/content/drive/My Drive/SentiHood/SentiHood Dataset/sentihood-test.json', 'r') as fp:\n","    testing_set = json.load(fp)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"pdHLlUXqUXm9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1596341111598,"user_tz":-330,"elapsed":358673,"user":{"displayName":"NIKHIL PRAKASH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE64-zyQNQSjSixENWJyhGqao6rzutp-TYuXNn2Q=s64","userId":"07464489639094286985"}},"outputId":"7a21103c-2fab-4f69-df38-33c5281dbeeb"},"source":["locations = ['LOCATION1', 'LOCATION2']\n","aspects = ['dining', 'general', 'green-nature', 'live', 'multicultural', 'nightlife', 'price', 'quiet', 'safety', 'shopping', 'touristy', 'transit-location']\n","\n","df = pd.DataFrame({'id': [], 'text': [], 'auxiliary_sentence': [], 'sentiment': []})\n","ii = 0\n","\n","count_location = 1\n","for location in locations:\n","  for each_example in tqdm(testing_set, ncols=80):\n","    id = str(int(each_example['id']))\n","    text = each_example['text'].strip()\n","\n","    # If `location` is present in the text, only then iterate over the  \n","    # list of opinions to find suitable `location-aspect` datapoints.\n","\n","    if location in text:\n","      # current_opinion_aspects: Is a dictionary containing all aspects and  \n","      # their corresponding sentiment present in the current opinion.\n","\n","      current_opinion_aspects = {}\n","      for opinion in each_example['opinions']:\n","        if opinion['target_entity'] == location:\n","          current_opinion_aspects[opinion['aspect']] = opinion['sentiment']\n","      \n","      text = text.replace(location, 'location - ' + str(count_location))\n","      \n","      for aspect in aspects:\n","        aspect_found = False\n","        \n","        for polarity in ['Positive', 'Negative']:\n","          auxiliary_sentence = f\"location - {str(count_location)} - {aspect} - {polarity}.\"\n","          \n","          if aspect in current_opinion_aspects.keys() and current_opinion_aspects[aspect] == polarity:\n","            df.loc[ii] = [id, text, auxiliary_sentence, int(1)]\n","            ii += 1\n","            aspect_found = True\n","          else:\n","            df.loc[ii] = [id, text, auxiliary_sentence, int(0)]\n","            ii += 1\n","        \n","        # If no sentiment is found for the `location-aspect`, then add the \n","        # auxiliary sentence with None to the dataset.\n","        \n","        auxiliary_sentence = f\"location - {str(count_location)} - {aspect} - None.\"\n","        if not aspect_found:\n","          df.loc[ii] = [id, text, auxiliary_sentence, int(1)]\n","          ii += 1\n","        else:\n","          df.loc[ii] = [id, text, auxiliary_sentence, int(0)]\n","          ii += 1\n","    \n","  count_location += 1"],"execution_count":5,"outputs":[{"output_type":"stream","text":["100%|███████████████████████████████████████| 1491/1491 [04:16<00:00,  5.80it/s]\n","100%|███████████████████████████████████████| 1491/1491 [01:40<00:00, 14.83it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"p58TqalzUtGI","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596341118798,"user_tz":-330,"elapsed":1417,"user":{"displayName":"NIKHIL PRAKASH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE64-zyQNQSjSixENWJyhGqao6rzutp-TYuXNn2Q=s64","userId":"07464489639094286985"}}},"source":["df['sentiment'] =  df['sentiment'].astype(int)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z_R37bEsWGFB","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596341155215,"user_tz":-330,"elapsed":3278,"user":{"displayName":"NIKHIL PRAKASH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgE64-zyQNQSjSixENWJyhGqao6rzutp-TYuXNn2Q=s64","userId":"07464489639094286985"}}},"source":["# df.to_csv('/content/drive/My Drive/SentiHood/Bert-pair/NLI-B/Datasets/testing_set.csv', index=False)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"niitrzEQWGzs","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}